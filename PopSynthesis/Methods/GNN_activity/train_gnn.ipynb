{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from PopSynthesis.Methods.GNN_activity.model import convert_to_temporal_data, GraphAttentionEmbedding, LinkPredictor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import TGNMemory, TransformerConv\n",
    "from torch_geometric.nn.models.tgn import IdentityMessage, LastAggregator, LastNeighborLoader\n",
    "from torch_geometric.loader import TemporalDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the starting and expected graphs\n",
    "data = torch.load(\"data/train_graph.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PyG HeteroData to TemporalData\n",
    "train_data = convert_to_temporal_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Model Configuration ===\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "memory_dim = time_dim = embedding_dim = 100\n",
    "\n",
    "memory = TGNMemory(\n",
    "    num_nodes=train_data.num_nodes,\n",
    "    raw_msg_dim=train_data.msg.size(-1),\n",
    "    memory_dim=memory_dim,\n",
    "    time_dim=time_dim,\n",
    "    message_module=IdentityMessage(train_data.msg.size(-1), memory_dim, time_dim),\n",
    "    aggregator_module=LastAggregator(),\n",
    ").to(device)\n",
    "\n",
    "gnn = GraphAttentionEmbedding(\n",
    "    in_channels=memory_dim,\n",
    "    out_channels=embedding_dim,\n",
    "    msg_dim=train_data.msg.size(-1),\n",
    "    time_enc=memory.time_enc,\n",
    ").to(device)\n",
    "\n",
    "link_pred = LinkPredictor(in_channels=embedding_dim).to(device)\n",
    "\n",
    "# === Optimizer & Loss ===\n",
    "optimizer = torch.optim.Adam(\n",
    "    set(memory.parameters()) | set(gnn.parameters()) | set(link_pred.parameters()), lr=0.0001)\n",
    "criterion_edge = torch.nn.BCEWithLogitsLoss()\n",
    "criterion_joint = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DataLoader ===\n",
    "train_loader = TemporalDataLoader(\n",
    "    train_data,\n",
    "    batch_size=200,\n",
    "    neg_sampling_ratio=1.0,\n",
    ")\n",
    "\n",
    "neighbor_loader = LastNeighborLoader(train_data.num_nodes, size=10, device=device)\n",
    "\n",
    "# === Training State Variables ===\n",
    "assoc = torch.empty(train_data.num_nodes, dtype=torch.long, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    memory.train()\n",
    "    gnn.train()\n",
    "    link_pred.train()\n",
    "\n",
    "    memory.reset_state()  # Fresh memory\n",
    "    neighbor_loader.reset_state()  # Empty graph at start\n",
    "\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Neighborhood sampling\n",
    "        n_id, edge_index, e_id = neighbor_loader(batch.n_id)\n",
    "        assoc[n_id] = torch.arange(n_id.size(0), device=device)\n",
    "\n",
    "        # Update memory\n",
    "        z, last_update = memory(n_id)\n",
    "        z = gnn(z, last_update, edge_index, train_data.t[e_id].to(device), train_data.msg[e_id].to(device))\n",
    "\n",
    "        # Positive & Negative samples\n",
    "        pos_edge_pred, pos_joint_pred = link_pred(z[assoc[batch.src]], z[assoc[batch.dst]])\n",
    "        neg_edge_pred, neg_joint_pred = link_pred(z[assoc[batch.src]], z[assoc[batch.neg_dst]])\n",
    "\n",
    "        # Labels\n",
    "        edge_labels = torch.cat([torch.ones_like(pos_edge_pred), torch.zeros_like(neg_edge_pred)])\n",
    "        edge_preds = torch.cat([pos_edge_pred, neg_edge_pred])\n",
    "\n",
    "        # Extract `joint_activity` from graph data\n",
    "        joint_labels = batch.msg[:, 0].view(-1, 1) # Joint is first\n",
    "        # Loss Calculation\n",
    "        loss_edge = criterion_edge(edge_preds, edge_labels)\n",
    "        loss_joint = criterion_joint(pos_joint_pred, joint_labels)  # Apply only on positive edges\n",
    "\n",
    "        loss = loss_edge + loss_joint  # Total loss\n",
    "\n",
    "        # Memory Update\n",
    "        memory.update_state(batch.src, batch.dst, batch.t, batch.msg)\n",
    "        neighbor_loader.insert(batch.src, batch.dst)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        memory.detach()\n",
    "        total_loss += float(loss) * batch.num_events\n",
    "\n",
    "    return total_loss / train_data.num_events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 1.1940\n",
      "Epoch: 02, Loss: 1.1913\n",
      "Epoch: 03, Loss: 1.1885\n",
      "Epoch: 04, Loss: 1.1858\n",
      "Epoch: 05, Loss: 1.1830\n",
      "Epoch: 06, Loss: 1.1801\n",
      "Epoch: 07, Loss: 1.1773\n",
      "Epoch: 08, Loss: 1.1744\n",
      "Epoch: 09, Loss: 1.1716\n",
      "Epoch: 10, Loss: 1.1688\n",
      "Epoch: 11, Loss: 1.1659\n",
      "Epoch: 12, Loss: 1.1631\n",
      "Epoch: 13, Loss: 1.1602\n",
      "Epoch: 14, Loss: 1.1573\n",
      "Epoch: 15, Loss: 1.1544\n",
      "Epoch: 16, Loss: 1.1515\n",
      "Epoch: 17, Loss: 1.1487\n",
      "Epoch: 18, Loss: 1.1458\n",
      "Epoch: 19, Loss: 1.1429\n",
      "Epoch: 20, Loss: 1.1399\n",
      "Epoch: 21, Loss: 1.1370\n",
      "Epoch: 22, Loss: 1.1341\n",
      "Epoch: 23, Loss: 1.1311\n",
      "Epoch: 24, Loss: 1.1281\n",
      "Epoch: 25, Loss: 1.1251\n",
      "Epoch: 26, Loss: 1.1220\n",
      "Epoch: 27, Loss: 1.1190\n",
      "Epoch: 28, Loss: 1.1158\n",
      "Epoch: 29, Loss: 1.1127\n",
      "Epoch: 30, Loss: 1.1095\n",
      "Epoch: 31, Loss: 1.1063\n",
      "Epoch: 32, Loss: 1.1031\n",
      "Epoch: 33, Loss: 1.0998\n",
      "Epoch: 34, Loss: 1.0965\n",
      "Epoch: 35, Loss: 1.0932\n",
      "Epoch: 36, Loss: 1.0899\n",
      "Epoch: 37, Loss: 1.0865\n",
      "Epoch: 38, Loss: 1.0831\n",
      "Epoch: 39, Loss: 1.0797\n",
      "Epoch: 40, Loss: 1.0762\n",
      "Epoch: 41, Loss: 1.0727\n",
      "Epoch: 42, Loss: 1.0691\n",
      "Epoch: 43, Loss: 1.0655\n",
      "Epoch: 44, Loss: 1.0619\n",
      "Epoch: 45, Loss: 1.0582\n",
      "Epoch: 46, Loss: 1.0546\n",
      "Epoch: 47, Loss: 1.0509\n",
      "Epoch: 48, Loss: 1.0472\n",
      "Epoch: 49, Loss: 1.0435\n",
      "Epoch: 50, Loss: 1.0397\n",
      "Epoch: 51, Loss: 1.0359\n",
      "Epoch: 52, Loss: 1.0320\n",
      "Epoch: 53, Loss: 1.0282\n",
      "Epoch: 54, Loss: 1.0244\n",
      "Epoch: 55, Loss: 1.0205\n",
      "Epoch: 56, Loss: 1.0166\n",
      "Epoch: 57, Loss: 1.0127\n",
      "Epoch: 58, Loss: 1.0087\n",
      "Epoch: 59, Loss: 1.0047\n",
      "Epoch: 60, Loss: 1.0007\n",
      "Epoch: 61, Loss: 0.9967\n",
      "Epoch: 62, Loss: 0.9926\n",
      "Epoch: 63, Loss: 0.9885\n",
      "Epoch: 64, Loss: 0.9843\n",
      "Epoch: 65, Loss: 0.9801\n",
      "Epoch: 66, Loss: 0.9759\n",
      "Epoch: 67, Loss: 0.9716\n",
      "Epoch: 68, Loss: 0.9673\n",
      "Epoch: 69, Loss: 0.9630\n",
      "Epoch: 70, Loss: 0.9585\n",
      "Epoch: 71, Loss: 0.9540\n",
      "Epoch: 72, Loss: 0.9495\n",
      "Epoch: 73, Loss: 0.9450\n",
      "Epoch: 74, Loss: 0.9403\n",
      "Epoch: 75, Loss: 0.9356\n",
      "Epoch: 76, Loss: 0.9308\n",
      "Epoch: 77, Loss: 0.9259\n",
      "Epoch: 78, Loss: 0.9210\n",
      "Epoch: 79, Loss: 0.9160\n",
      "Epoch: 80, Loss: 0.9109\n",
      "Epoch: 81, Loss: 0.9058\n",
      "Epoch: 82, Loss: 0.9007\n",
      "Epoch: 83, Loss: 0.8955\n",
      "Epoch: 84, Loss: 0.8903\n",
      "Epoch: 85, Loss: 0.8850\n",
      "Epoch: 86, Loss: 0.8796\n",
      "Epoch: 87, Loss: 0.8742\n",
      "Epoch: 88, Loss: 0.8687\n",
      "Epoch: 89, Loss: 0.8631\n",
      "Epoch: 90, Loss: 0.8574\n",
      "Epoch: 91, Loss: 0.8517\n",
      "Epoch: 92, Loss: 0.8459\n",
      "Epoch: 93, Loss: 0.8401\n",
      "Epoch: 94, Loss: 0.8342\n",
      "Epoch: 95, Loss: 0.8283\n",
      "Epoch: 96, Loss: 0.8224\n",
      "Epoch: 97, Loss: 0.8164\n",
      "Epoch: 98, Loss: 0.8104\n",
      "Epoch: 99, Loss: 0.8043\n",
      "Epoch: 100, Loss: 0.7983\n"
     ]
    }
   ],
   "source": [
    "# === Run Training ===\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    print(f\"Epoch: {epoch:02d}, Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "popsyn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
